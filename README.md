# ğŸ“˜ Loss Landscape Geometry & Optimization Dynamics in Neural Networks  
### *A Comparative Study of Batch-Normalized and Non-Normalized CNNs*

This repository contains the full implementation and report for the project **â€œLoss Landscape Geometry and Optimization Dynamics in Neural Networksâ€**.  
The goal is to analyze how **neural network architecture choices**â€”particularly **Batch Normalization (BN)**â€”affect the loss landscape, optimization behavior, and generalization.

This work was completed as part of a research-oriented machine learning assignment, and includes both **theoretical foundations** and **empirical experiments**.

---

## ğŸ“„ **Project Overview**
Modern neural networks operate in extremely high-dimensional, non-convex loss landscapes.  
Yet, stochastic gradient descent (SGD) consistently finds solutions that generalize well.

This project explores **why**, by analyzing:

- Loss surface curvature  
- Sharpness under perturbations  
- Local geometry using 1D loss slices  
- Basin topology via mode connectivity  
- The effect of Batch Normalization on landscape geometry  

Two CNNs with identical architectures were trained on **Fashion-MNIST**, differing only by the inclusion of **BatchNorm**.

---


---

## ğŸ§  **Theoretical Highlights**

The analysis includes:

### ğŸ”¹ Loss Geometry  
- Hessian curvature  
- Gradient behavior  
- Local quadratic approximations  

### ğŸ”¹ Perturbation Sharpness  
Sharpness measured using **SAM-style adversarial perturbations**.

### ğŸ”¹ SGD as a Stochastic Differential Equation  
Understanding why SGD prefers **broader minima**:

\[
p(\theta) \propto \frac{e^{-L(\theta)/T}}{\sqrt{\det H(\theta)}}
\]

Shows how temperature/noise affects optimization trajectories.

---

## ğŸ§ª **Experimental Setup**

- **Dataset:** Fashion-MNIST  
- **Models:**  
  - A1: CNN + BatchNorm  
  - A2: CNN + BatchNorm (different seed)  
  - B: CNN without BatchNorm  
- **Optimizer:** SGD + Momentum  
- **Epochs:** 6  
- **Batch size:** 128  

Both models share the same architecture; BN is the only difference.

---

## ğŸ“Š **Key Results** (with figures)

### ğŸ”¹ 1. Training & Test Performance  
BatchNorm improves both training stability and generalization.

**Model A1 (with BN)**  
![Model A1](figures/P1A.png)

**Model B (no BN)**  
![Model B](figures/P1B.png)

---

### ğŸ”¹ 2. Curvature (Hessian Eigenvalue)
\[
\lambda_{\max}(A1) \approx 57.18,\qquad
\lambda_{\max}(B) \approx 37.51
\]

BN â†’ **higher curvature**, yet **better generalization**.

---

### ğŸ”¹ 3. Perturbation Sharpness
\[
S(A1) \approx 0.1837,\qquad
S(B) \approx 0.0886
\]

BN appears **sharper**, contradicting the naive â€œflat minima generalize bestâ€ claim.

---

### ğŸ”¹ 4. 1D Loss Slice  
Shows local geometry around the minima.

![1D Slice](figures/P2.png)

---

### ğŸ”¹ 5. Mode Connectivity (A1 â†” A2)  
Linear interpolation reveals a **loss barrier**, meaning the minima lie in different basins.

![Mode Connectivity](figures/P3.png)

---

## ğŸ§© **Discussion: What We Learned**

### â­ BatchNorm generalizes better **despite** being sharper  
This challenges the classical belief that flat minima generalize better.

### â­ Local curvature â‰  global geometry  
Curvature metrics like Î»_max or SAM sharpness are **incomplete** descriptors.

### â­ Basin topology matters  
A1 and A2 minima are separated by a loss barrier â†’ distinct basins.

### â­ Architecture strongly shapes optimization  
BatchNorm influences:
- gradient flow  
- curvature  
- sensitivity  
- basin structure  

---

## ğŸ **Conclusion**

This project provides a structured and empirical analysis of how architectural choicesâ€”specifically **Batch Normalization**â€”shape the loss landscape in deep networks.  
While BN improves optimization speed and generalization, it also **increases curvature and sharpness**, demonstrating that generalization depends on far more than simply â€œflatnessâ€.

The work emphasizes the importance of:
- combining multiple geometric probes,  
- understanding global topology, and  
- analyzing the interaction between architecture and optimization noise.

## ğŸ“ **Full Report**

For complete theoretical derivations, plots, and explanations, see:

ğŸ“„ **[DA24M014_REPORT.pdf](DA24M014_REPORT.pdf)**  

---

## ğŸ‘¤ **Author**
**Rajat Abhijit Kambale**

---




